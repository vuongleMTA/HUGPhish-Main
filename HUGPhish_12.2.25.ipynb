{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d1dabd84-d826-462d-8672-6c50b95672f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TODO: use manual HTML features\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from utils import run_ML\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "from itertools import groupby\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1fd1cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import run_LGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2362c08a-bc2b-4b59-952a-9303ecdea868",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import codecs\n",
    "import gensim\n",
    "import urllib\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from bs4 import BeautifulSoup\n",
    "from numpy import load, save,  asarray\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import networkx as nx\n",
    "from scipy.sparse import csr_matrix\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29f29055-f291-41fb-8bb2-f0dfa71b1759",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# tmp_dir = \"E:/Phising URL Detection/Baseline/DLM-main/DLM-main/tmp\"\n",
    "data_dir = \"E:/Phising URL Detection/Methods/html_files/html_files\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a412cfc-46d1-4227-8ed5-7d2ecd92d8a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "doc2vec = gensim.models.doc2vec.Doc2Vec.load(\"E:/Phising URL Detection/Baseline/DLM-main/DLM-main/doc2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "190a4016-74ab-4e74-946a-7775cde2ac0d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from utils import generate_filename, convert, read_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6a0e1699-73e3-4864-ac7b-4372b03b94de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def callTag(tag, node):\n",
    "    node_from = node\n",
    "    node = node + 1\n",
    "\n",
    "    for tag in tag.children:\n",
    "        attr_node = 1\n",
    "        if (tag.name is not None):\n",
    "            nodes_list.append(convert([str(node_from), str(node)]))\n",
    "            attr_list.append(str(node) + \",nname,\" + str(tag.name))\n",
    "            attr_list.append(str(node) + \",value,\" + \"\")\n",
    "            for attr in tag.attrs:\n",
    "                nodes_list.append(convert([str(node), str(node) + \"_\" + str(attr_node)]))\n",
    "                attr_list.append(str(node) + \"_\" + str(attr_node) + \",nname,\" + str(attr))\n",
    "                attr_list.append(str(node) + \"_\" + str(attr_node) + \",value,\" + str(tag.get(attr)))\n",
    "                attr_node = attr_node + 1\n",
    "            callTag(tag, node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "716dcd3a-17a1-40dd-9e3a-70af987fb029",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_udst_inputs(url, rec_id, phishing_flag):\n",
    "    node = 0\n",
    "    f = codecs.open(Path(data_dir + '/' + generate_filename(rec_id)), 'r', encoding='utf-8', errors='ignore')\n",
    "    soup = BeautifulSoup(f)\n",
    "    callTag(soup, node)\n",
    "\n",
    "    nodes_list.pop(0)\n",
    "    attr_list.pop(1)\n",
    "    attr_list.insert(1, '1, value, ' + url)\n",
    "\n",
    "    G = nx.Graph()\n",
    "    G.add_edges_from(nodes_list)\n",
    "\n",
    "    A = nx.adjacency_matrix(G) # N*N Adjacency matrix\n",
    "\n",
    "    df_features = pd.DataFrame(0.0, index=G.nodes(), columns=['nname', 'value'])\n",
    "\n",
    "    attr_val_list = []\n",
    "    for x in attr_list:\n",
    "        y = x.split(',')\n",
    "        if y[1] == \"value\":\n",
    "            attr_val_list.append(y[2])\n",
    "\n",
    "    for x in attr_list:\n",
    "        y = x.split(',')\n",
    "        test_corpus = list(read_corpus([y[2]], tokens_only=True))\n",
    "        vector = doc2vec.infer_vector(test_corpus[0])\n",
    "        # print(test_corpus[0])\n",
    "\n",
    "        if y[1] == \"nname\":\n",
    "            df_features.loc[y[0]][\"nname\"] = vector\n",
    "\n",
    "        else:\n",
    "            df_features.loc[y[0]][\"value\"] = vector\n",
    "\n",
    "    X = df_features.values # N*d Feature Matrix\n",
    "    # y = to_categorical(phishing_flag, num_classes=2).tolist() # Label\n",
    "    y = phishing_flag\n",
    "\n",
    "    X_ = np.array([np.array(ai, dtype=np.float32) for ai in X.tolist()])\n",
    "\n",
    "    # url_token = url_tokenizer([url])[0]\n",
    "    url_token = None\n",
    "\n",
    "    return X_, A, y, url_token, url"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9132d59-ef74-44d2-91aa-e89d9b5fc54d",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "51996577-859a-466e-a40f-b60f6a51a8d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a143c217",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('E:/Phising URL Detection/Methods/Dataset/concat.csv')\n",
    "df['result'] = df['result'].replace({'legitimate': 0, 'phishing': 1})\n",
    "smalldata=df\n",
    "labels=df['result']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b1191cc9-98c5-44a7-8c0c-29f92121e08d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rec_id</th>\n",
       "      <th>url</th>\n",
       "      <th>result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>579</td>\n",
       "      <td>https://www.pedromonjo.com/</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>765</td>\n",
       "      <td>http://forallstudodupdafor.co.nf/_about_.html</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rec_id                                            url  result\n",
       "0     579                    https://www.pedromonjo.com/       0\n",
       "1     765  http://forallstudodupdafor.co.nf/_about_.html       1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smalldata.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fcd08658-b0b2-44af-8058-95c255eed904",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Counter({0: 1496, 1: 735}), 2231)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import collections\n",
    "counter = collections.Counter(labels)\n",
    "counter, len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "323dea5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1784, 447, 1784, 447)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Chia dữ liệu thành chỉ số train và test\n",
    "train_idx = list(range(0, 1784))  # Từ 0 đến 1783\n",
    "test_idx = list(range(1784, 2231))  # Từ 1784 đến 2230\n",
    "\n",
    "train_data = smalldata.iloc[train_idx]\n",
    "test_data = smalldata.iloc[test_idx]\n",
    "\n",
    "# Tạo các biến chứa labels cho train và test\n",
    "train_labels = labels.iloc[train_idx]\n",
    "test_labels = labels.iloc[test_idx]\n",
    "\n",
    "# Hiển thị kích thước của dữ liệu train và test\n",
    "len(train_data), len(test_data), len(train_labels), len(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa0a837d-63e3-40cf-afc9-9593072d34e4",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Conventional Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3cd8d0e0-af10-42b5-9bd9-bced95415a22",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from utils import extract_features_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bb80b3c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'domain': 'www.klikaj.hr',\n",
       " 'num_subdomains': 2,\n",
       " 'contains_ip': 0,\n",
       " 'path_length': 1,\n",
       " 'num_path_segments': 1,\n",
       " 'uses_https': 1,\n",
       " 'file_extension': '',\n",
       " 'count_special_characters': 6,\n",
       " 'count_non_alphanumeric_characters': 6,\n",
       " 'TLD': 'hr',\n",
       " 'count_obfuscated_characters': 0,\n",
       " 'letter_ratio_in_url': 0.7272727272727273,\n",
       " 'digit_ratio_in_url': 0.0,\n",
       " 'count_equals_in_url': 0,\n",
       " 'NoOfAmpersandInURL': 0,\n",
       " 'CharContinuationRate': 0.18181818181818182,\n",
       " 'ratio_obfuscated_characters': 0.0,\n",
       " 'NoOfQMarkInURL': 0}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_features_url(\"https://www.klikaj.hr/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c999c69c-2338-4ea3-a5df-2bbd10c0dd49",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## test on numerical URLs features\n",
    "from utils import extract_numerical_features\n",
    "phish_url = []\n",
    "for link in list(smalldata.iloc[:,1]):\n",
    "    url_features = extract_numerical_features(link)\n",
    "    phish_url.append(list(url_features.values()))\n",
    "# run_ML(np.array(phish_url), labels, \"URLdataset13.2\", \"url_manual_numerical\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7fcdb818",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1784, 447)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Chia phish_url thành train và test theo train_idx và test_idx\n",
    "train_phish_url = np.array(phish_url)[train_idx]\n",
    "test_phish_url = np.array(phish_url)[test_idx]\n",
    "\n",
    "# Kiểm tra kích thước của train_phish_url và test_phish_url\n",
    "len(train_phish_url), len(test_phish_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0af8bb19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running LightGBM...\n",
      "F1 Score: 0.8207\n",
      "Precision Score: 0.8244\n",
      "Recall Score: 0.8174\n"
     ]
    }
   ],
   "source": [
    "run_LGB(train_phish_url, train_labels, test_phish_url, test_labels,\"concat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed85c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import lightgbm as lgb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef79b7fe-f017-4894-a558-ecf95e00c992",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Extract graph features from URLs for PyG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2be46503-1dab-4c89-9a14-2859346a62ac",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8853669795353132\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>CountVectorizer(analyzer=&#x27;char&#x27;, ngram_range=(1, 4))</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CountVectorizer</label><div class=\"sk-toggleable__content\"><pre>CountVectorizer(analyzer=&#x27;char&#x27;, ngram_range=(1, 4))</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "CountVectorizer(analyzer='char', ngram_range=(1, 4))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer\n",
    "# List of URLs\n",
    "urls = list(smalldata['url'])\n",
    "# Tokenization and N-grams Generation\n",
    "# You can adjust ngram_range to extract different n-grams (e.g., (1, 1) for unigrams, (2, 2) for bigrams, etc.)\n",
    "vectorizer = CountVectorizer(analyzer='char', ngram_range=(1, 4)) #5\n",
    "X_counts = vectorizer.fit_transform(urls)\n",
    "# TF-IDF Transformation\n",
    "transformer = TfidfTransformer()\n",
    "X_tfidf = transformer.fit_transform(X_counts)\n",
    "# Extracted Features\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "X_counts_data = X_counts.toarray() # not necessary\n",
    "# Train lgb\n",
    "model_lgb = lgb.LGBMClassifier(verbose=-1)\n",
    "model_lgb.fit(X_counts_data[train_idx], labels[train_idx])\n",
    "y_predict=model_lgb.predict(X_counts_data[test_idx]) \n",
    "print(f1_score(y_predict, labels[test_idx], average='macro'))\n",
    "feature_imp_gain = pd.DataFrame(sorted(zip(model_lgb.booster_.feature_importance(importance_type='gain'),\n",
    "                                           feature_names), reverse=True), columns=['Value', 'Feature'])\n",
    "feature_imp_split = pd.DataFrame(sorted(zip(model_lgb.booster_.feature_importance(importance_type='split'),\n",
    "                                            feature_names), reverse=True), columns=['Value', 'Feature'])\n",
    "top_ngrams_features = list(set(list(feature_imp_gain.iloc[:200,1]) + list(feature_imp_split.iloc[:200,1])))\n",
    "cv = CountVectorizer(analyzer='char', ngram_range=(1, 4))\n",
    "cv.fit(top_ngrams_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8ec2dd2e-e01b-4b9d-bb3e-f8a36ec4e787",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_feature_CountVectorizer(model, url):\n",
    "    return model.transform([url]).toarray().flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1ddb6f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract countvectorizer feature of all URLs\n",
    "phish_url_countvectorizer = []\n",
    "for link in list(smalldata.iloc[:,1]):\n",
    "    url_features = extract_feature_CountVectorizer(cv,link)\n",
    "    phish_url_countvectorizer.append(url_features)\n",
    "# run_ML(np.array(phish_url_countvectorizer), labels, \"URLdataset13.2\", \"ngramURLfeatures\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2aff0302-b0ae-4da5-9f44-171a1010f762",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from requests.exceptions import ConnectionError\n",
    "import traceback\n",
    "import logging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e10ff57d-4c7d-452c-980a-9e64ab31812b",
   "metadata": {},
   "source": [
    "# Run HTML features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8d84036c-da1d-4c9d-99e7-d35fd9548004",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Idea: Combine with manual features from HTMLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d1a612b8-8a9b-4092-bb92-edcdcf2e2b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HTML offline\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "# import whois\n",
    "from urllib.parse import urlparse\n",
    "from collections import Counter\n",
    "\n",
    "# Function to parse HTML content from a URL\n",
    "def get_html_content(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Raise error if the response is bad\n",
    "        return response.text\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching URL {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Function to extract HTML structure features\n",
    "def extract_structure_features(soup):\n",
    "    features = {}\n",
    "    \n",
    "    features['num_divs'] = len(soup.find_all('div'))\n",
    "    features['num_scripts'] = len(soup.find_all('script'))\n",
    "    features['num_forms'] = len(soup.find_all('form'))\n",
    "    features['num_links'] = len(soup.find_all('a'))\n",
    "    features['num_iframes'] = len(soup.find_all('iframe'))\n",
    "    \n",
    "    # Text to HTML ratio\n",
    "    text_length = len(soup.get_text())\n",
    "    html_length = len(str(soup))\n",
    "    features['text_to_html_ratio'] = text_length / html_length if html_length > 0 else 0\n",
    "    \n",
    "    return features\n",
    "\n",
    "# Function to extract text-based features\n",
    "def extract_text_features(soup):\n",
    "    \n",
    "    features = {}\n",
    "\n",
    "    # Title tag content\n",
    "    title = soup.title.string if soup.title and soup.title.string else ''\n",
    "    features['title_length'] = len(title)\n",
    "\n",
    "    # Visible text length\n",
    "    visible_text = soup.get_text() if soup else ''\n",
    "    features['visible_text_length'] = len(visible_text)\n",
    "\n",
    "    return features\n",
    "\n",
    "# Function to extract link-based features\n",
    "# def extract_link_features(soup, base_url):\n",
    "#     features = {}\n",
    "    \n",
    "#     all_links = soup.find_all('a', href=True)\n",
    "#     internal_links = [link for link in all_links if urlparse(link['href']).netloc == urlparse(base_url).netloc]\n",
    "#     external_links = [link for link in all_links if urlparse(link['href']).netloc != urlparse(base_url).netloc]\n",
    "    \n",
    "#     features['num_internal_links'] = len(internal_links)\n",
    "#     features['num_external_links'] = len(external_links)\n",
    "    \n",
    "#     return features\n",
    "\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "def extract_link_features(soup, base_url):\n",
    "    features = {}\n",
    "    \n",
    "    try:\n",
    "        all_links = soup.find_all('a', href=True)\n",
    "        internal_links = []\n",
    "        external_links = []\n",
    "        \n",
    "        for link in all_links:\n",
    "            href = link['href']\n",
    "            try:\n",
    "                parsed_href = urlparse(href)\n",
    "                parsed_base = urlparse(base_url)\n",
    "                \n",
    "                if parsed_href.netloc == parsed_base.netloc:\n",
    "                    internal_links.append(href)\n",
    "                else:\n",
    "                    external_links.append(href)\n",
    "            except ValueError:\n",
    "                continue  # Skip invalid links\n",
    "        \n",
    "        features['num_internal_links'] = len(internal_links)\n",
    "        features['num_external_links'] = len(external_links)\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting link features: {e}\")\n",
    "        features['num_internal_links'] = 0\n",
    "        features['num_external_links'] = 0\n",
    "    \n",
    "    return features\n",
    "\n",
    "\n",
    "# Function to extract JavaScript-based features\n",
    "def extract_js_features(soup):\n",
    "    features = {}\n",
    "    \n",
    "    script_tags = soup.find_all('script')\n",
    "    features['num_js_files'] = len(script_tags)\n",
    "    \n",
    "    # Suspicious JavaScript patterns\n",
    "    suspicious_js_patterns = ['eval', 'document.write', 'window.location']\n",
    "    features['suspicious_js_count'] = sum(\n",
    "        any(p in script.get_text() for p in suspicious_js_patterns) for script in script_tags)\n",
    "    \n",
    "    return features\n",
    "\n",
    "# Function to extract all features from a URL\n",
    "def extract_features(url, rec_id):\n",
    "#     html_content = get_html_content(url)\n",
    "#     if not html_content:\n",
    "#         return None\n",
    "    \n",
    "#     soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    try:\n",
    "        f = codecs.open(Path(data_dir + '/' + generate_filename(rec_id)), 'r', encoding='utf-8', errors='ignore')\n",
    "        soup = BeautifulSoup(f)\n",
    "        \n",
    "        # Extract all feature sets\n",
    "        structure_features = extract_structure_features(soup)\n",
    "        text_features = extract_text_features(soup)\n",
    "        link_features = extract_link_features(soup, url)\n",
    "        js_features = extract_js_features(soup)\n",
    "        # domain_features = extract_domain_features(url)\n",
    "        \n",
    "        # Combine all features into a single dictionary\n",
    "        features = {**structure_features, **text_features, **link_features, **js_features}\n",
    "    except:\n",
    "        print()\n",
    "        print(f\"Lỗi tại {rec_id}\")\n",
    "        features = [0]*12\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b69085d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'num_divs': 0,\n",
       " 'num_scripts': 1,\n",
       " 'num_forms': 0,\n",
       " 'num_links': 0,\n",
       " 'num_iframes': 0,\n",
       " 'text_to_html_ratio': 0.0,\n",
       " 'title_length': 0,\n",
       " 'visible_text_length': 0,\n",
       " 'num_internal_links': 0,\n",
       " 'num_external_links': 0,\n",
       " 'num_js_files': 1,\n",
       " 'suspicious_js_count': 0}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_features('https://dressxsozswlzs4khuybvx6b7ur4a-gtr8-ja92z-p4te-yy23.4everland.app/cio.html', 7608)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9c56de6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'num_divs': 426,\n",
       " 'num_scripts': 7,\n",
       " 'num_forms': 0,\n",
       " 'num_links': 121,\n",
       " 'num_iframes': 0,\n",
       " 'text_to_html_ratio': 0.23880716044000447,\n",
       " 'title_length': 24,\n",
       " 'visible_text_length': 44917,\n",
       " 'num_internal_links': 39,\n",
       " 'num_external_links': 79,\n",
       " 'num_js_files': 7,\n",
       " 'suspicious_js_count': 0}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_features(\"http://vamoaestudiarmedicina.blogspot.com/\", 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0e260c80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2010"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smalldata.iloc[159,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "fcbde0cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.kotikokki.net/'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smalldata.iloc[159,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ae0459",
   "metadata": {},
   "outputs": [],
   "source": [
    "sma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "90da06a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,\n",
      "Lỗi tại 1080\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'values'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m rec_id \u001b[38;5;241m=\u001b[39m smalldata\u001b[38;5;241m.\u001b[39miloc[idx, \u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m      6\u001b[0m features \u001b[38;5;241m=\u001b[39m extract_features(url, rec_id)\n\u001b[1;32m----> 7\u001b[0m manual_html_features\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mlist\u001b[39m(\u001b[43mfeatures\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m()))\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'values'"
     ]
    }
   ],
   "source": [
    "manual_html_features = []\n",
    "for idx in range(len(smalldata.index)):\n",
    "    url = smalldata.iloc[idx, 1]\n",
    "    print(idx,end=',')\n",
    "    rec_id = smalldata.iloc[idx, 0]\n",
    "    features = extract_features(url, rec_id)\n",
    "    manual_html_features.append(list(features.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1afc7671",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(manual_html_features[212])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a4d2e2b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7608"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smalldata.iloc[213,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "73f92e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# # Lưu vào file pickle\n",
    "# with open(\"manual_html_features_concat.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(manual_html_features, f)\n",
    "\n",
    "# Mở file pickle và nạp dữ liệu vào biến manual_html_features\n",
    "with open(\"manual_html_features_concat.pkl\", \"rb\") as f:\n",
    "    manual_html_features= pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ab8ce0a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2231"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(manual_html_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0521c214",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running LightGBM...\n",
      "F1 Score: 0.8457\n",
      "Precision Score: 0.8451\n",
      "Recall Score: 0.8463\n"
     ]
    }
   ],
   "source": [
    "run_LGB(np.array(manual_html_features)[train_idx], train_labels, np.array(manual_html_features)[test_idx], test_labels,\"concat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b67482b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ngramUrl_manualHtml_features = np.concatenate([phish_url_countvectorizer, manual_html_features], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d99a0af2-dbb5-4486-827e-6701f53505c0",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Create dataset class for PyG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e55784ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "566cd482",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'smalldata' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m graphs \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      3\u001b[0m labels_list \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;28mlen\u001b[39m(\u001b[43msmalldata\u001b[49m\u001b[38;5;241m.\u001b[39mindex)):\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# for i in range(2):\u001b[39;00m\n\u001b[0;32m      6\u001b[0m     nodes_list \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      7\u001b[0m     attr_list \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[1;31mNameError\u001b[0m: name 'smalldata' is not defined"
     ]
    }
   ],
   "source": [
    "if True: #don't delete this\n",
    "    graphs = []\n",
    "    labels_list = []\n",
    "    for i in range(0,len(smalldata.index)):\n",
    "    # for i in range(2):\n",
    "        nodes_list = []\n",
    "        attr_list = []\n",
    "        print(i, end = ',')\n",
    "        X, A, y, X_A, _ = get_udst_inputs(smalldata.iloc[i, 1], smalldata.iloc[i, 0], smalldata.iloc[i, 2])\n",
    "\n",
    "        child_id, source_id = A.nonzero()\n",
    "        edge_index = torch.tensor([source_id,\n",
    "                                   child_id], dtype=torch.long)\n",
    "        x = torch.tensor(X, dtype=torch.float)\n",
    "        y = torch.tensor([labels[i]], dtype=torch.int64)\n",
    "        data = Data(x=x, edge_index=edge_index, y = y)\n",
    "        graphs.append(data)\n",
    "        labels_list.append(labels[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a80b5f86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2231"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(graphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0dedc028",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_list = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "23e24a6b-5a65-41f7-968a-5c7d01e6f750",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# with open(\"graph_html_9.3\", \"wb\") as fp:   #Pickling\n",
    "#     pickle.dump(graphs, fp)\n",
    "with open(\"graph_html_9.3\", \"rb\") as fp:   # Unpickling\n",
    "    graphs = pickle.load(fp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "989171c6-277c-434e-83a9-b618267e54a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train freq:  [1496, 735]\n"
     ]
    }
   ],
   "source": [
    "print(\"Train freq: \", [len(list(group)) for key, group in groupby(sorted(labels_list))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "95aeabfb-454b-4984-a875-a6104537382d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data, Dataset\n",
    "\n",
    "class GraphClassificationDataset(Dataset):\n",
    "    def __init__(self, graphs):\n",
    "        self.graphs = graphs\n",
    "        # self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.graphs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        graph = self.graphs[idx]\n",
    "        # label = self.labels[idx]\n",
    "        return graph\n",
    "    \n",
    "    def get(): pass\n",
    "\n",
    "    def len(): pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e607ec88-7a07-476f-a644-96c494bed7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = GraphClassificationDataset(graphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "56107391-7b84-4614-b367-81d5d04b0545",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset: GraphClassificationDataset(2231):\n",
      "====================\n",
      "Number of graphs: 2231\n",
      "Number of features: 2\n",
      "Number of classes: 2\n"
     ]
    }
   ],
   "source": [
    "print()\n",
    "print(f'Dataset: {dataset}:')\n",
    "print('====================')\n",
    "print(f'Number of graphs: {len(dataset)}')\n",
    "print(f'Number of features: {dataset.num_features}')\n",
    "print(f'Number of classes: {dataset.num_classes}')\n",
    "\n",
    "data = dataset[0]  # Get the first graph object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c34344fc-0487-4632-8e65-676e88e892c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset = dataset[:int(0.8*n_samples)]\n",
    "# test_dataset = dataset[int(0.8*n_samples):]\n",
    "train_dataset = [dataset[idx] for idx in train_idx]\n",
    "test_dataset = [dataset[idx] for idx in test_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "fb440b6e-47cc-4b2e-a554-e6ff6fec2bfb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1784, 447)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset), len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "acf1c066-f7b9-4f66-8260-330c70c40068",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8729a4ea-1ac8-4ea5-97ef-8fedbbf3311a",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Build and train PyG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5b7fba98-98b6-4a62-8917-8a5d10f2d02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.nn import global_mean_pool, global_max_pool\n",
    "\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super(GCN, self).__init__()\n",
    "        torch.manual_seed(12345)\n",
    "        self.conv1 = GCNConv(dataset.num_node_features, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.conv3 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.lin = Linear(hidden_channels, dataset.num_classes)\n",
    "        self.linconcat = Linear(2*hidden_channels, dataset.num_classes)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        # 1. Obtain node embeddings \n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.conv3(x, edge_index)\n",
    "\n",
    "        # 2. Readout layer\n",
    "        x = global_mean_pool(x, batch)  # [batch_size, hidden_channels]\n",
    "\n",
    "        # 3. Apply a final classifier\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.lin(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a874e18f-2693-4626-a66c-15952318b86b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 000, Train F1: 0.5733, Test F1: 0.5870\n",
      "Epoch: 010, Train F1: 0.6419, Test F1: 0.6173\n"
     ]
    }
   ],
   "source": [
    "model = GCN(hidden_channels=64)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "\n",
    "    for data in train_loader:  # Iterate in batches over the training dataset.\n",
    "        # print(data.x.shape)\n",
    "        out = model(data.x, data.edge_index, data.batch)  # Perform a single forward pass.\n",
    "        loss = criterion(out, data.y)  # Compute the loss.\n",
    "        loss.backward()  # Derive gradients.\n",
    "        optimizer.step()  # Update parameters based on gradients.\n",
    "        optimizer.zero_grad()  # Clear gradients.\n",
    "\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    # for data in loader:  # Iterate in batches over the training/test dataset.\n",
    "    #     out = model(data.x, data.edge_index, data.batch)  \n",
    "    #     pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
    "    #     correct += int((pred == data.y).sum())  # Check against ground-truth labels.\n",
    "    # return correct / len(loader.dataset)  # Derive ratio of correct predictions.\n",
    "    true_labels = []\n",
    "    pred_labels = []\n",
    "    for data in loader:  # Iterate in batches over the training/test dataset.\n",
    "        out = model(data.x, data.edge_index, data.batch)  \n",
    "        pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
    "        correct += int((pred == data.y).sum())  # Check against ground-truth labels.\n",
    "        true_labels += data.y.tolist()\n",
    "        pred_labels += pred.tolist()\n",
    "        # print(pred_labels)\n",
    "    return f1_score(true_labels, pred_labels, average='macro')\n",
    "\n",
    "for epoch in range(0, 20):\n",
    "    train()\n",
    "    train_acc = test(train_loader)\n",
    "    test_acc = test(test_loader)\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch: {epoch:03d}, Train F1: {train_acc:.4f}, Test F1: {test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f69e8d-74fc-4369-aabb-e8d4b723a1ee",
   "metadata": {
    "tags": []
   },
   "source": [
    "### GNN for dim. reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00edafd-7277-4644-b563-2910d441a5ee",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Net 1: 0.87, 0.88"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c74a0be3-48cf-4b24-9c56-48826ab199d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.nn import global_mean_pool, global_max_pool\n",
    "\n",
    "class GCNdimReduce(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super(GCNdimReduce, self).__init__()\n",
    "        torch.manual_seed(12345)\n",
    "        self.conv1 = GCNConv(dataset.num_node_features, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.conv3 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.lin1 = Linear(hidden_channels, hidden_channels)\n",
    "        self.lin2 = Linear(hidden_channels, dataset.num_classes)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        # 1. Obtain node embeddings \n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.conv3(x, edge_index)\n",
    "\n",
    "        # 2. Readout layer\n",
    "        x = global_mean_pool(x, batch)  # [batch_size, hidden_channels]\n",
    "\n",
    "        # 3. Apply a final classifier\n",
    "        x = self.lin1(x)\n",
    "        x = F.dropout(x, p=0.3, training=self.training)\n",
    "        x = self.lin2(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def dimReduce(self, x, edge_index, batch):\n",
    "        # 1. Obtain node embeddings \n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.conv2(x, edge_index)\n",
    "        # x = x.relu()\n",
    "        # x = self.conv3(x, edge_index)\n",
    "\n",
    "        # 2. Readout layer\n",
    "        x = global_mean_pool(x, batch)  # [batch_size, hidden_channels]\n",
    "\n",
    "        # # 3. Apply a final classifier\n",
    "        x = self.lin1(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc9cc7d-4f1a-476f-ab19-2643c4a404e7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Net 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a751eb5f-e5eb-4450-8ba0-c65ef45d9486",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.nn import global_mean_pool, global_max_pool\n",
    "\n",
    "class GCNdimReduceV2(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super(GCNdimReduceV2, self).__init__()\n",
    "        torch.manual_seed(12345)\n",
    "        self.conv1 = GCNConv(dataset.num_node_features, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.conv3 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.lin1 = Linear(hidden_channels, hidden_channels)\n",
    "        self.lin2 = Linear(hidden_channels, dataset.num_classes)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        # 1. Obtain node embeddings \n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.conv3(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.conv3(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = x.relu()\n",
    "       \n",
    "        # 2. Readout layer\n",
    "        x = global_mean_pool(x, batch)  # [batch_size, hidden_channels]\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        # 3. Apply a final classifier\n",
    "        x = self.lin1(x)\n",
    "        x = x.relu()\n",
    "        x = self.lin2(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def dimReduce(self, x, edge_index, batch):\n",
    "        # 1. Obtain node embeddings \n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.conv3(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.conv3(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = global_mean_pool(x, batch)  # [batch_size, hidden_channels]\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.lin1(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "717a4558-f031-44f1-9a1d-4780580cae3b",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "bcd6b977-ad35-43eb-adbc-d401261e6d2c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 000, Train F1: 0.4013, Test F1: 0.4016\n",
      "Epoch: 001, Train F1: 0.4719, Test F1: 0.4630\n",
      "Epoch: 002, Train F1: 0.5325, Test F1: 0.5409\n",
      "Epoch: 003, Train F1: 0.5827, Test F1: 0.5800\n",
      "Epoch: 004, Train F1: 0.6470, Test F1: 0.6260\n",
      "Epoch: 005, Train F1: 0.6255, Test F1: 0.6008\n",
      "Epoch: 006, Train F1: 0.6303, Test F1: 0.5990\n",
      "Epoch: 007, Train F1: 0.6408, Test F1: 0.6123\n",
      "Epoch: 008, Train F1: 0.6374, Test F1: 0.6086\n",
      "Epoch: 009, Train F1: 0.6423, Test F1: 0.6183\n"
     ]
    }
   ],
   "source": [
    "n_hidden_channels = 16\n",
    "model = GCNdimReduce(hidden_channels=n_hidden_channels)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "\n",
    "    for data in train_loader:  # Iterate in batches over the training dataset.\n",
    "        # print(data.x.shape)\n",
    "        out = model(data.x, data.edge_index, data.batch)  # Perform a single forward pass.\n",
    "        loss = criterion(out, data.y)  # Compute the loss.\n",
    "        loss.backward()  # Derive gradients.\n",
    "        optimizer.step()  # Update parameters based on gradients.\n",
    "        optimizer.zero_grad()  # Clear gradients.\n",
    "\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    true_labels = []\n",
    "    pred_labels = []\n",
    "    for data in loader:  # Iterate in batches over the training/test dataset.\n",
    "        out = model(data.x, data.edge_index, data.batch)  \n",
    "        pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
    "        correct += int((pred == data.y).sum())  # Check against ground-truth labels.\n",
    "        true_labels += data.y.tolist()\n",
    "        pred_labels += pred.tolist()\n",
    "    return f1_score(true_labels, pred_labels, average='macro'), precision_score(true_labels, pred_labels, average='macro'), recall_score(true_labels, pred_labels, average='macro')\n",
    "\n",
    "\n",
    "for epoch in range(0, 10):\n",
    "    train()\n",
    "    train_f1, train_pre, train_rec = test(train_loader)\n",
    "    test_f1, test_pre, test_rec = test(test_loader)\n",
    "    if epoch % 1 == 0:\n",
    "        print(f'Epoch: {epoch:03d}, Train F1: {train_f1:.4f}, Test F1: {test_f1:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a05648-8619-48c4-9f1f-32100030cb2a",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Def"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b89c35c1-ac25-430e-b212-87492087a072",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_model(all_data_loader, train_loader, test_loader, n_hidden_channels = 16, n_epoch=1, lr=0.001):\n",
    "    # n_hidden_channels = 16\n",
    "    model = GCNdimReduce(hidden_channels=n_hidden_channels)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    def train():\n",
    "        model.train()\n",
    "\n",
    "        for data in train_loader:  # Iterate in batches over the training dataset.\n",
    "            # print(data.x.shape)\n",
    "            out = model(data.x, data.edge_index, data.batch)  # Perform a single forward pass.\n",
    "            loss = criterion(out, data.y)  # Compute the loss.\n",
    "            loss.backward()  # Derive gradients.\n",
    "            optimizer.step()  # Update parameters based on gradients.\n",
    "            optimizer.zero_grad()  # Clear gradients.\n",
    "\n",
    "    def test(loader):\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        true_labels = []\n",
    "        pred_labels = []\n",
    "        for data in loader:  # Iterate in batches over the training/test dataset.\n",
    "            out = model(data.x, data.edge_index, data.batch)  \n",
    "            pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
    "            correct += int((pred == data.y).sum())  # Check against ground-truth labels.\n",
    "            true_labels += data.y.tolist()\n",
    "            pred_labels += pred.tolist()\n",
    "        return f1_score(true_labels, pred_labels, average='macro'), precision_score(true_labels, pred_labels, average='macro'), recall_score(true_labels, pred_labels, average='macro')\n",
    "\n",
    "\n",
    "    for epoch in range(0, n_epoch):\n",
    "        train()\n",
    "        train_f1, train_pre, train_rec = test(train_loader)\n",
    "        test_f1, test_pre, test_rec = test(test_loader)\n",
    "        if epoch % 1 == 0:\n",
    "            print(f'Epoch: {epoch:03d}, Train F1: {train_f1:.4f}, Test F1: {test_f1:.4f}')\n",
    "    \n",
    "    model.eval()\n",
    "    dim_vec = torch.empty((0, n_hidden_channels), dtype=torch.float32)\n",
    "    for data in all_data_loader:\n",
    "        dim_x = model.dimReduce(data.x, data.edge_index, data.batch)\n",
    "        dim_vec = torch.cat((dim_vec, dim_x), 0)\n",
    "    return (dim_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ac5bad79-d5d8-46ea-9935-52c6050c7a1b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 000, Train F1: 0.4013, Test F1: 0.4016\n"
     ]
    }
   ],
   "source": [
    "all_data_loader = DataLoader(dataset, batch_size=1, shuffle=False)\n",
    "dim_vec = train_model(all_data_loader, train_loader, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "53c3fadc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2231"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "420834c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_dir=\"results\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "309c61c6-f8fa-4c06-999f-576892f31bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def runHUGPHISH(train_idx, test_idx):\n",
    "    # stack_GNNs_graph = np.concatenate((np.array(phish_url_vectorizer), hyperlink_features),axis=1)\n",
    "    stack_GNNs_graph = np.array(ngramUrl_manualHtml_features)\n",
    "    for i in range(10):\n",
    "        np.random.seed(i) \n",
    "        # train_idx_new = list(np.random.choice(list(range(n_samples)), int(0.8*n_samples), replace=False))\n",
    "        # train_idx_new = list(set(train_idx_new).difference(test_idx))\n",
    "        train_idx_new = list(np.random.choice(train_idx, int(0.8*len(train_idx)), replace=False))\n",
    "        print(train_idx_new[:5])\n",
    "        train_dataset_new = [dataset[idx] for idx in train_idx_new]\n",
    "        train_loader_new = DataLoader(train_dataset_new, batch_size=8, shuffle=True)\n",
    "        # test_dataset = [dataset[idx] for idx in test_idx]\n",
    "        # test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "        n_hidden_channels = 6\n",
    "        n_epoch = 1\n",
    "        dim_vec_new = train_model(all_data_loader, train_loader_new, test_loader, n_hidden_channels, n_epoch)\n",
    "        stack_GNNs_graph = np.concatenate((stack_GNNs_graph, dim_vec_new.detach().numpy()),axis=1)\n",
    "    \n",
    "    model_lgb2 = lgb.LGBMClassifier(verbose=-1)\n",
    "    model_lgb2.fit(stack_GNNs_graph[train_idx], labels[train_idx])\n",
    "    y_predict=model_lgb2.predict(stack_GNNs_graph[test_idx]) \n",
    "    f1 = f1_score(test_labels, y_predict, average='macro')\n",
    "    pre = precision_score(test_labels, y_predict, average='macro')\n",
    "    rec = recall_score(test_labels, y_predict, average='macro')\n",
    "    print(f1, pre, rec)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c622dfbd-432c-4d04-9226-9d80fc03684d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1010, 942, 1183, 963, 1422]\n",
      "Epoch: 000, Train F1: 0.4012, Test F1: 0.4016\n",
      "[1546, 851, 302, 108, 1719]\n",
      "Epoch: 000, Train F1: 0.4017, Test F1: 0.4016\n",
      "[612, 505, 502, 799, 1089]\n",
      "Epoch: 000, Train F1: 0.3997, Test F1: 0.4016\n",
      "[610, 889, 1767, 774, 361]\n",
      "Epoch: 000, Train F1: 0.4007, Test F1: 0.4016\n",
      "[674, 1753, 256, 956, 1061]\n",
      "Epoch: 000, Train F1: 0.4044, Test F1: 0.4016\n",
      "[120, 1467, 1266, 628, 1432]\n",
      "Epoch: 000, Train F1: 0.4017, Test F1: 0.4016\n",
      "[1435, 649, 88, 140, 702]\n",
      "Epoch: 000, Train F1: 0.4064, Test F1: 0.4016\n",
      "[58, 281, 1102, 452, 788]\n",
      "Epoch: 000, Train F1: 0.4002, Test F1: 0.4016\n",
      "[1566, 195, 1537, 1520, 1679]\n",
      "Epoch: 000, Train F1: 0.4027, Test F1: 0.4016\n",
      "[1525, 301, 412, 404, 607]\n",
      "Epoch: 000, Train F1: 0.4039, Test F1: 0.4016\n",
      "0.9117642082466202 0.9097306397306397 0.9139115646258503\n"
     ]
    }
   ],
   "source": [
    "runHUGPHISH(train_idx, test_idx)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graphish_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
